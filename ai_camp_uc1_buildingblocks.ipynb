{
  "cells": [
    {
      "cell_type": "code",
      "id": "xQuLv4UBymt9AkrKkJxeX45F",
      "metadata": {
        "tags": [],
        "id": "xQuLv4UBymt9AkrKkJxeX45F"
      },
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Beginner's Guide to Key Functionality"
      ],
      "metadata": {
        "id": "pEqxse4Q3xLO"
      },
      "id": "pEqxse4Q3xLO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Laxmi Harikumar](https://github.com/laxmi-genai) |"
      ],
      "metadata": {
        "id": "6QHBcxKp39Kj"
      },
      "id": "6QHBcxKp39Kj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "[Gemini 2.0 Flash-Lite](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2#2.0-flash-lite) is a new multimodal generative ai model from the Gemini family developed by [Google DeepMind](https://deepmind.google/). It is Google's fastest and most cost efficient Flash model. It's an upgrade path for 1.5 Flash users who want better quality for the same price and speed.\n",
        "\n",
        "Gemini 2.0 Flash-Lite includes:\n",
        "\n",
        "- Multimodal input, text output\n",
        "- 1M token input context window\n",
        "- 8k token output context window\n",
        "\n",
        "### Pricing\n",
        "Information on the pricing for Gemini 2.0 Flash is available on our [Pricing page](https://cloud.google.com/vertex-ai/generative-ai/pricing).\n",
        "\n",
        "\n",
        "## Objective\n",
        "\n",
        "In this notebook, you'll explore the examples for:\n",
        "\n",
        "- Controlled Generation\n",
        "- Automated Function Calling"
      ],
      "metadata": {
        "id": "hW6KYN9d4Pu0"
      },
      "id": "hW6KYN9d4Pu0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started"
      ],
      "metadata": {
        "id": "9X48mzDE4SxL"
      },
      "id": "9X48mzDE4SxL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Google Gen AI SDK for Python"
      ],
      "metadata": {
        "id": "b6duqgPA4Yle"
      },
      "id": "b6duqgPA4Yle"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet google-genai"
      ],
      "metadata": {
        "id": "P0SSojk03k-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16d1f6e-aa65-43e7-965f-01881941f060"
      },
      "id": "P0SSojk03k-j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ],
      "metadata": {
        "id": "a0QKXKnPwd6d"
      },
      "id": "a0QKXKnPwd6d"
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0YsawYywfxC",
        "outputId": "a219a989-39b5-4f04-f773-661d6f59a23b"
      },
      "id": "R0YsawYywfxC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>"
      ],
      "metadata": {
        "id": "ok75nhe4wl7o"
      },
      "id": "ok75nhe4wl7o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ],
      "metadata": {
        "id": "LIbWAHSQ4f4b"
      },
      "id": "LIbWAHSQ4f4b"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n"
      ],
      "metadata": {
        "id": "5PwSy7x84iwv"
      },
      "id": "5PwSy7x84iwv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "ZqTGK_HW5pxl"
      },
      "id": "ZqTGK_HW5pxl"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    GenerateContentConfig,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    Content\n",
        ")\n",
        "import os"
      ],
      "metadata": {
        "id": "1qSXm6VA5gTQ"
      },
      "id": "1qSXm6VA5gTQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Google Cloud project information and create client\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ],
      "metadata": {
        "id": "lr-RM9st5wez"
      },
      "id": "lr-RM9st5wez"
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")"
      ],
      "metadata": {
        "id": "sA7oNG5k5xZ4"
      },
      "id": "sA7oNG5k5xZ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "eo0dgrcZ6AsL"
      },
      "id": "eo0dgrcZ6AsL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the Gemini 2.0 Flash-Lite model"
      ],
      "metadata": {
        "id": "lg6-5agi6EiZ"
      },
      "id": "lg6-5agi6EiZ"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-lite-preview-02-05\"  # @param {type: \"string\"}"
      ],
      "metadata": {
        "id": "--R5sma_6I3W"
      },
      "id": "--R5sma_6I3W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Safety Filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the Configure safety filters page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "You can use safety_settings to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to BLOCK_LOW_AND_ABOVE for all categories:"
      ],
      "metadata": {
        "id": "kDmycZtS6z_U"
      },
      "id": "kDmycZtS6z_U"
    },
    {
      "cell_type": "code",
      "source": [
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "t-uUqysX6-z3"
      },
      "id": "t-uUqysX6-z3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters)."
      ],
      "metadata": {
        "id": "Wvfzc4up6fW7"
      },
      "id": "Wvfzc4up6fW7"
    },
    {
      "cell_type": "code",
      "source": [
        "generate_content_config=GenerateContentConfig(\n",
        "        temperature=0.4,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        safety_settings=safety_settings\n",
        "    )"
      ],
      "metadata": {
        "id": "-n3O0JTQ6eMI"
      },
      "id": "-n3O0JTQ6eMI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Controlled Generation\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`."
      ],
      "metadata": {
        "id": "AqH6rP4O7fjc"
      },
      "id": "AqH6rP4O7fjc"
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a popular cookie recipe and its ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "V-KWC9nDzWQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d1963b-534f-4164-daa6-a8b8e2575534"
      },
      "id": "V-KWC9nDzWQw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"description\": \"This is a classic chocolate chip cookie recipe.\",\n",
            "  \"ingredients\": [\n",
            "    \"1 cup (2 sticks) unsalted butter, softened\",\n",
            "    \"3/4 cup granulated sugar\",\n",
            "    \"3/4 cup packed brown sugar\",\n",
            "    \"1 teaspoon vanilla extract\",\n",
            "    \"2 large eggs\",\n",
            "    \"2 1/4 cups all-purpose flour\",\n",
            "    \"1 teaspoon baking soda\",\n",
            "    \"1 teaspoon salt\",\n",
            "    \"2 cups chocolate chips\"\n",
            "  ],\n",
            "  \"name\": \"Chocolate Chip Cookies\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
      ],
      "metadata": {
        "id": "0NENfhGf0ZRD"
      },
      "id": "0NENfhGf0ZRD"
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_response: Recipe = response.parsed\n",
        "print(parsed_response)"
      ],
      "metadata": {
        "id": "oDBUkt_k0aBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961a90b2-c427-4686-8410-5c53d5316121"
      },
      "id": "oDBUkt_k0aBE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Chocolate Chip Cookies' description='This is a classic chocolate chip cookie recipe.' ingredients=['1 cup (2 sticks) unsalted butter, softened', '3/4 cup granulated sugar', '3/4 cup packed brown sugar', '1 teaspoon vanilla extract', '2 large eggs', '2 1/4 cups all-purpose flour', '1 teaspoon baking soda', '1 teaspoon salt', '2 cups chocolate chips']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Controlled generation supports the following fields from the [Vertex AI schema](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output). If you use an unsupported field, Vertex AI can still handle your request but ignores the field.\n",
        "\n",
        "- `anyOf`\n",
        "- `enum`\n",
        "- `format`\n",
        "- `items`\n",
        "- `maximum`\n",
        "- `maxItems`\n",
        "- 'minimum`\n",
        "- 'minItems`\n",
        "- 'nullable`\n",
        "- 'properties`\n",
        "- 'required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format."
      ],
      "metadata": {
        "id": "E4cc9qvO0ekM"
      },
      "id": "E4cc9qvO0ekM"
    },
    {
      "cell_type": "code",
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response_dict = response.parsed\n",
        "display(response_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "1gnAgxiH7jIw",
        "outputId": "e1f2493e-84c1-4e8b-8511-c36c11961e08"
      },
      "id": "1gnAgxiH7jIw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[[{'explanation': 'The review expresses strong positive feelings and high satisfaction.',\n",
              "   'flavor': 'Strawberry Cheesecake',\n",
              "   'rating': 4,\n",
              "   'sentiment': 'POSITIVE'},\n",
              "  {'explanation': 'The review indicates a somewhat positive experience with a minor negative aspect.',\n",
              "   'flavor': 'Mango Tango',\n",
              "   'rating': 1,\n",
              "   'sentiment': 'NEGATIVE'}]]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with."
      ],
      "metadata": {
        "id": "1f2XeAugriZd"
      },
      "id": "1f2XeAugriZd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Function (Automatic Function Calling)"
      ],
      "metadata": {
        "id": "iHVd_KeZ002j"
      },
      "id": "iHVd_KeZ002j"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Chicago, Il\": \"windy\",\n",
        "    }\n",
        "\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in San Francisco?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4l_Z4pn28PJx",
        "outputId": "d2b8eb8a-166b-4fc0-f962-2588dedb6ce4"
      },
      "id": "4l_Z4pn28PJx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The weather in San Francisco is foggy.\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "ai_camp_uc1_buildingblocks"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}